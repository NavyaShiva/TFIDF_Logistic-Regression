{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "67kBL_ebyTGk",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "#                          IDS 561 Assignment 2                                 | \n",
        "#==============================================================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EAfNbpzPNSHX",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "#      WARNING - Run this only if you want to clear all the variable values     | \n",
        "#===============================================================================\n",
        "%reset -f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RNs9jqLkyXGe",
        "outputId": "ef21676a-e2d4-466d-c5ab-bf1cee99c5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#===============================================================================\n",
        "#                   Importing all the required libraries                        |\n",
        "#===============================================================================\n",
        "\n",
        "import   pandas                        as pd\n",
        "import   numpy                         as np\n",
        "\n",
        "import nltk\n",
        "from   nltk.tokenize                   import WhitespaceTokenizer\n",
        "from   nltk.stem                       import PorterStemmer \n",
        "from   nltk.tokenize                   import word_tokenize \n",
        "from   nltk.corpus                     import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import string\n",
        "\n",
        "from   sklearn.feature_extraction.text import TfidfTransformer\n",
        "from   sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LdLrk-91yZ_l",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "#                         Read the Training Data                                |\n",
        "#===============================================================================\n",
        "\n",
        "#Reading the input CSV file into a DataFrame\n",
        "file_train = pd.read_csv('train_data.csv',encoding = \"ISO-8859-1\")\n",
        "\n",
        "#Picking the required columns i.e. Sentiment and Tweet\n",
        "c = [1,3]\n",
        "file_train = file_train.iloc[:,c]\n",
        "file_train.rename(columns={'SentimentText':'Tweet'}, inplace=True)\n",
        "#file_train = file_train.iloc[:,:]\n",
        "\n",
        "\n",
        "#===============================================================================\n",
        "#                         Read the Test Data                                    |\n",
        "#===============================================================================\n",
        "\n",
        "#Reading the input CSV file into a DataFrame\n",
        "file_test = pd.read_csv('train_data.csv',encoding = \"ISO-8859-1\")\n",
        "\n",
        "#Picking the required columns i.e. Sentiment and Tweet\n",
        "c = [1,3]\n",
        "file_test = file_test.iloc[:,c]\n",
        "file_test.rename(columns={'SentimentText':'Tweet'}, inplace=True)\n",
        "\n",
        "#file_test = file_test.iloc[1:50,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "74zlfxc24_y1",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "# Train Data Cleaning- Removing Punctuations, Stop words and making Lowercase   |\n",
        "#===============================================================================\n",
        "\n",
        "train_file = file_train\n",
        "\n",
        "#Removing Punctuations from the Tweet column in dataframe\n",
        "train_file['Tweet'] = train_file['Tweet'].str.replace('[^\\w\\s]','')\n",
        "#Converting all the words in Tweet column to lowercase\n",
        "train_file['Tweet'] = train_file['Tweet'].str.lower()\n",
        "\n",
        "#Removing all the numbers from the strings in dataFrame\n",
        "train_file['Tweet'] = train_file['Tweet'].str.replace('\\d+', '')\n",
        "\n",
        "#Removing all the stopwords from the Tweet column in dataframe\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "train_file['Tweet'] = train_file['Tweet'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
        "\n",
        "#===============================================================================\n",
        "# Test Data Cleaning- Removing Punctuations, Stop words and making Lowercase    |\n",
        "#===============================================================================\n",
        "\n",
        "test_file = file_test\n",
        "\n",
        "#Removing Punctuations from the Tweet column in dataframe\n",
        "test_file['Tweet'] = test_file['Tweet'].str.replace('[^\\w\\s]','')\n",
        "#Converting all the words in Tweet column to lowercase\n",
        "test_file['Tweet'] = test_file['Tweet'].str.lower()\n",
        "\n",
        "#Removing all the numbers from the strings in dataFrame\n",
        "test_file['Tweet'] = test_file['Tweet'].str.replace('\\d+', '')\n",
        "\n",
        "#Removing all the stopwords from the Tweet column in dataframe\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "test_file['Tweet'] = test_file['Tweet'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JcB_oKHQ82LE",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "#                         TF-IDF for Train Data                                 |\n",
        "#===============================================================================\n",
        "\n",
        "train_tfidf = train_file\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "v = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "#Vectorizer to get TF-IDF of the Tweet column\n",
        "train_tfidf = v.fit_transform(train_tfidf['Tweet'])\n",
        "\n",
        "#Convert the above TF-IDF vectorizer to a Pandas Dataframe\n",
        "train_tfidf = pd.DataFrame(data = train_tfidf.toarray(), columns=v.get_feature_names())\n",
        "\n",
        "#Fetching the sentiment column from the original dataframe\n",
        "Tweet_train = train_file.iloc[:,0]\n",
        "\n",
        "#Correcting the index of the column with index starting from 0\n",
        "Tweet_train.index       = np.arange(0, len(Tweet_train)) \n",
        "train_tfidf.index = np.arange(0, len(train_tfidf))\n",
        "\n",
        "#Now insert the sentiment column in the first column to the above TF-IDF Dataframe\n",
        "train_tfidf.insert(0, \"Tweet\", Tweet_train)\n",
        "\n",
        "#===============================================================================\n",
        "#                         TF-IDF for Test Data                                  |\n",
        "#===============================================================================\n",
        "\n",
        "test_tfidf = test_file\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "v = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "#Vectorizer to get TF-IDF of the Tweet column\n",
        "test_tfidf = v.fit_transform(test_tfidf['Tweet'])\n",
        "\n",
        "#Convert the above TF-IDF vectorizer to a Pandas Dataframe\n",
        "test_tfidf = pd.DataFrame(data = test_tfidf.toarray(), columns=v.get_feature_names())\n",
        "\n",
        "#Fetching the sentiment column from the original dataframe\n",
        "Tweet_test = test_file.iloc[:,0]\n",
        "\n",
        "#Correcting the index of the column with index starting from 0\n",
        "Tweet_test.index = np.arange(0, len(Tweet_test)) \n",
        "\n",
        "#Now insert the sentiment column in the first column to the above TF-IDF Dataframe\n",
        "test_tfidf.insert(0, \"Tweet\", Tweet_test)\n",
        "\n",
        "#train_tfidf - Contains the Train data with Tweet column as Target feature\n",
        "#test_tfidf  - Contains the Test data  with Tweet column as Target feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GEPvyTW-BcL6",
        "colab": {}
      },
      "source": [
        "#Input Data to the Logistic Regression Model\n",
        "X_train = train_tfidf\n",
        "X_TRAIN = X_train.iloc[:,1:]\n",
        "\n",
        "Y_train = train_tfidf.iloc[:,0]\n",
        "Y_TRAIN = Y_train.to_frame()\n",
        "\n",
        "X_TEST  = test_tfidf.iloc[:,1:]\n",
        "Y_TEST  = test_tfidf.iloc[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C_jQ02cYiGfr",
        "outputId": "0def921d-2c15-40d1-d3fd-d770dca5399d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#===============================================================================\n",
        "#                         START OF LOGISTIC REGRESSION                          |\n",
        "#===============================================================================\n",
        "\n",
        "#STEP 1: Splitting the data into 10 Folds\n",
        "\n",
        "#STEP 2: For each fold, based on 100 iterations we find the theta values\n",
        "#        after using Gradient Descent          \n",
        "\n",
        "#STEP 3: After finding the theta values for each fold, based on those values\n",
        "#        we predict on the Validation data\n",
        "\n",
        "#STEP 4: We find the Accuracy for that fold using those theta values\n",
        "\n",
        "#STEP 5: We repeat this process for all 10 folds and find the BEST accuracy\n",
        "\n",
        "#STEP 6: Whichever fold has the best accuracy, we pick the corresponding\n",
        "#        theta values as the final theta values \n",
        "    \n",
        "#STEP 7: The final theta values will be used for the final predictions\n",
        "#===============================================================================\n",
        "\n",
        "#K-Fold Cross Validation\n",
        "from numpy import array\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "#Prepare cross validation\n",
        "kfold = KFold(10, True, 1)\n",
        "bestaccuracy = 0\n",
        "theta_final = np.zeros(X_TRAIN.shape[1])\n",
        "\n",
        "# Enumerate splits\n",
        "for train, test in kfold.split(X_TRAIN):\n",
        "    X_train    = X_TRAIN.iloc[train,:]\n",
        "    X_validate = X_TRAIN.iloc[test,:]\n",
        "    \n",
        "    Y_train    = Y_TRAIN.iloc[train,:]\n",
        "    Y_validate = Y_TRAIN.iloc[test,:]\n",
        "    \n",
        "    X = X_train\n",
        "    y = Y_train\n",
        "\n",
        "    num_iter = 5\n",
        "\n",
        "    for i in range(num_iter):\n",
        "  \n",
        "      #PART 1 : Sigmoid Function\n",
        "      #Input Parameters - X and theta\n",
        "\n",
        "      #Create a Zero values matrix whose dimensions are equal to the number of columns in Dataframe\n",
        "\n",
        "      if 'theta_new' not in vars():  #Only if theta_new is blank fill theta values with 0's\n",
        "        theta = np.zeros(X.shape[1]) #The feature coefficient matrix\n",
        "  \n",
        "      z = np.dot(X, theta)           #Z = X.Î¸ Matrix Multiplication (i.e. Row is multipled by Columns)\n",
        "      h = 1 / (1 + np.exp(-z))       #Appling Sigmoid function on X\n",
        "      h = h.reshape(len(X),1)\n",
        "\n",
        "      #PART 2 : Find the Gradient of Loss Function\n",
        "\n",
        "      gradient = np.dot(X.T, (h - y)) / y.shape[0]\n",
        "\n",
        "      learning_rate = 0.1\n",
        "      weight        = theta\n",
        "      theta_new     = weight - learning_rate * gradient\n",
        "  \n",
        "      theta_new     = pd.DataFrame(theta_new)\n",
        "      theta_new     = theta_new.iloc[:,0].to_frame()\n",
        "      theta         = theta_new \n",
        "    \n",
        "    del theta_new #Clearing the variable after each fold\n",
        "\n",
        "    #PART 3 : Making the Predictions on the Validation Data\n",
        "\n",
        "    z    = np.dot(X_validate, theta) #Applying sigmoid on the new Theta values\n",
        "    pred = 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    #PART 4 : Calculating the Accuracy based on the actual target values\n",
        "\n",
        "    predicted_class = ((pred >= 0.5) .astype(int))\n",
        "    predicted_class = predicted_class.flatten()\n",
        "    predicted_class = predicted_class.reshape(len(predicted_class),1)\n",
        "    acc             = np.mean(predicted_class == Y_validate)\n",
        "    \n",
        "    #PART 5 : Finding the final theta value for the best accuracy obtained fold\n",
        "     \n",
        "    if(acc[0] > bestaccuracy):\n",
        "        theta_final  = theta\n",
        "        bestaccuracy = acc[0]\n",
        "\n",
        "print('Best Accuracy: {0:0.2f}'.format(bestaccuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Accuracy: 0.71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "56YzA51_uCbD",
        "colab": {}
      },
      "source": [
        "#Final Predictions based on the final theta values obtained\n",
        "\n",
        "z    = np.dot(X_TEST, theta_final) #Applying sigmoid on the new Theta values\n",
        "pred = 1 / (1 + np.exp(-z))\n",
        "\n",
        "predicted_class = ((pred >= 0.5) .astype(int))\n",
        "predicted_class = predicted_class.flatten()    #Like unlist in R\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_fYhNmSjv6uq",
        "outputId": "18d80c41-3a04-4510-c378-d078a78b3ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#Calculating the Recall, Precision and Average Precision-recall score\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "average_precision = average_precision_score(Y_TEST, predicted_class)\n",
        "\n",
        "prec = precision_score(Y_TEST, predicted_class)\n",
        "recall = recall_score(Y_TEST, predicted_class)\n",
        "conf = confusion_matrix(Y_TEST, predicted_class)\n",
        "\n",
        "print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
        "\n",
        "print('Precision score: {0:0.2f}'.format(prec))\n",
        "\n",
        "print('Recall score: {0:0.2f}'.format(recall))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average precision-recall score: 0.68\n",
            "Precision score: 0.70\n",
            "Recall score: 0.86\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}